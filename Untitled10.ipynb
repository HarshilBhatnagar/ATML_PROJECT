{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTh1YCjYAq-5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data = pd.read_csv('/kaggle/input/pheme-dataset-for-rumour-detection/dataset.csv')\n",
        "data\n",
        "\n",
        "data.info()\n",
        "\n",
        "data.shape\n",
        "\n",
        "# Data Preprocessing\n",
        "# Handle missing values in 'is_rumor' column by dropping or filling them\n",
        "data = data.dropna(subset=['is_rumor'])\n",
        "data\n",
        "\n",
        "# Filter the DataFrame to only include rows where 'is_rumor' is 1 or 0\n",
        "filtered_data = data[data['is_rumor'].isin([1, 0])]\n",
        "\n",
        "# Count the values of 'is_rumor'\n",
        "rumor_counts = filtered_data['is_rumor'].value_counts()\n",
        "\n",
        "print(\"Count of is_rumor values:\")\n",
        "print(rumor_counts)\n",
        "\n",
        "# Convert text to lowercase and remove unnecessary characters\n",
        "data.loc[:, 'text'] = data['text'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
        "data['text']\n",
        "\n",
        "# Ensure 'is_rumor' is integer type\n",
        "data.loc[:, 'is_rumor'] = data['is_rumor'].astype(int)\n",
        "data['is_rumor']\n",
        "\n",
        "# Train-Test Split\n",
        "X = data[['text']]\n",
        "y = data['is_rumor']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorization and Feature Union\n",
        "# Text vectorization\n",
        "text_transformer = Pipeline(steps=[\n",
        "    ('vectorizer', TfidfVectorizer(stop_words='english'))\n",
        "])\n",
        "\n",
        "# Scaling numerical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine text and numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', text_transformer, 'text')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Model Pipeline\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression( max_iter=1000))\n",
        "])\n",
        "\n",
        "# Model Training\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy:\", (accuracy_score(y_test, predictions).round(2)) * 100,'%')\n",
        "\n",
        "# Define the metrics list\n",
        "metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "# Prepare data for plotting\n",
        "values_class_0 = [report['0.0'][metric] for metric in metrics] + [accuracy]\n",
        "values_class_1 = [report['1.0'][metric] for metric in metrics] + [accuracy]\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "bar_width = 0.2\n",
        "opacity = 0.8\n",
        "index = range(len(metrics) + 1)\n",
        "\n",
        "rects1 = ax.bar([p - bar_width/2 for p in index], values_class_0, bar_width, alpha=opacity, color='darkblue', label='Not Rumor')\n",
        "rects2 = ax.bar([p + bar_width/2 for p in index], values_class_1, bar_width, alpha=opacity, color='gray', label='Rumor')\n",
        "\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Classification Metrics and Accuracy by Class')\n",
        "ax.set_xticks(index)\n",
        "ax.set_xticklabels(metrics + ['Accuracy'])\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Rumor', 'Rumor'], yticklabels=['Not Rumor', 'Rumor'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}